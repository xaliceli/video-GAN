{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video-gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6Ja32ubOA6n4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Video GAN\n"
      ]
    },
    {
      "metadata": {
        "id": "8ayRtp_AWe-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8bosNVk3cdFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eOnuODD1ArGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ]
    },
    {
      "metadata": {
        "id": "tOVBnAFniibX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Video read settings\n",
        "VIDEO_DIR = '/content/drive/My Drive/Colab Data/video-gan'\n",
        "INPUT_SIZE = (240, 320)\n",
        "VIDEO_SIZE = (64, 64)\n",
        "FRAME_INT = 1\n",
        "FRAME_CAP = 32\n",
        "\n",
        "# Training parameters\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1000\n",
        "Z_DIM = 100\n",
        "DISC_ITERATIONS = 5\n",
        "GEN_ITERATIONS = 1\n",
        "\n",
        "# Adam optimizer\n",
        "LEARNING_RATE = 0.0002\n",
        "BETA1 = 0.5\n",
        "\n",
        "# Output frequency\n",
        "SAMPLE_RATE = 100\n",
        "SAVE_RATE = 50\n",
        "NUM_OUT = 1\n",
        "\n",
        "# Use eager execution\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PdUyvKGHSt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Video Processing"
      ]
    },
    {
      "metadata": {
        "id": "NX7WdeMsL9st",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract frames"
      ]
    },
    {
      "metadata": {
        "id": "fyesgxzfMu2R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "videos = glob.glob(os.path.join(VIDEO_DIR, 'inputs/*.avi'))\n",
        "all_image_paths = glob.glob(os.path.join(VIDEO_DIR, 'inputs/*.jpg'))\n",
        "\n",
        "if len(videos) > len(all_image_paths):\n",
        "    # For each video in directory, capture every frame_int number of frames and \n",
        "    # store in array where each frame is stacked horizontally.\n",
        "    for vnum, video in enumerate(videos):\n",
        "        description = os.path.splitext(video)[0]\n",
        "        vidcap = cv2.VideoCapture(os.path.join(VIDEO_DIR, video))\n",
        "        success, image = vidcap.read()\n",
        "        output = np.zeros((FRAME_CAP * image.shape[0], image.shape[1], image.shape[2]))\n",
        "        loc, frames = 0, 0\n",
        "        while success and frames < FRAME_CAP:\n",
        "            output[frames * image.shape[0]:(frames + 1) * image.shape[0]] = image\n",
        "            loc += FRAME_INT\n",
        "            frames += 1\n",
        "            vidcap.set(cv2.CAP_PROP_POS_MSEC, loc)\n",
        "            success, image = vidcap.read()\n",
        "        INPUT_SIZE = image.shape[:2]\n",
        "        if frames == FRAME_CAP:\n",
        "            cv2.imwrite(os.path.join(VIDEO_DIR, 'inputs', description + str(vnum) + '.jpg'), np.float32(output))\n",
        "    vidcap.release()\n",
        "    all_image_paths = glob.glob(os.path.join(VIDEO_DIR, 'inputs/*.jpg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIWfnu_ML_3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Read frames into tf data object"
      ]
    },
    {
      "metadata": {
        "id": "7pOYKYozHWuL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reads video image, decodes into tensor, resized to desired shape.\n",
        "def parse_video(filename):\n",
        "    image_string = tf.read_file(filename)\n",
        "    image_decoded = tf.cast(tf.image.decode_jpeg(image_string, channels=3), tf.float32)\n",
        "    frames = tf.reshape(image_decoded, [-1, INPUT_SIZE[0], INPUT_SIZE[1], 3])\n",
        "    image_resized = tf.image.resize_images(frames, VIDEO_SIZE)\n",
        "    return tf.subtract(tf.math.divide(image_resized, 127.5), 1.0)\n",
        "\n",
        "# File name vector.\n",
        "num_use = int(len(all_image_paths)/BATCH_SIZE)*BATCH_SIZE\n",
        "all_image_paths = [str(path) for path in all_image_paths[:num_use]]\n",
        "\n",
        "# Construct dataset.\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_image_paths).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.map(parse_video).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifvlQ0T6A2YG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ]
    },
    {
      "metadata": {
        "id": "C7hiV9hzTK2Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_avi(frames, directory, name='', frate=24):\n",
        "    writer = cv2.VideoWriter(os.path.join(directory, name), \n",
        "                             cv2.VideoWriter_fourcc('X', 'V', 'I', 'D'),\n",
        "                             frate, VIDEO_SIZE)\n",
        "    frames_concat = None\n",
        "    for f_num, frame in enumerate(frames):\n",
        "        if frames_concat is None:\n",
        "            frames_concat = frame[0]\n",
        "        else:\n",
        "            frames_concat = np.hstack((frames_concat, frame[0]))\n",
        "        writer.write(frame[0])\n",
        "    cv2.imwrite(os.path.join(directory, 'epoch' + name + '.jpg'), frames_concat)\n",
        "    writer.release()\n",
        "\n",
        "    \n",
        "def convert_image(images, samples):\n",
        "    images = tf.cast(np.clip(((images + 1.0) * 127.5), 0, 255), tf.uint8)\n",
        "    images = [tf.squeeze(image).numpy() for image in tf.split(images, samples, axis=1)]\n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxRCYsGWA0Qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "GYqdBq3biAqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VideoGAN():\n",
        "  \n",
        "    def __init__(self,\n",
        "                 input,\n",
        "                 batch_size,\n",
        "                 num_frames,\n",
        "                 crop_size,\n",
        "                 learning_rate,\n",
        "                 z_dim,\n",
        "                 conv_init,\n",
        "                 beta1,\n",
        "                 disc_iterations,\n",
        "                 gen_iterations,\n",
        "                 num_out,\n",
        "                 epochs,\n",
        "                 save_int,\n",
        "                 save_dir,\n",
        "                 save_checkpts=True):\n",
        "        self.videos = input\n",
        "        self.batch_size = batch_size\n",
        "        self.num_frames = num_frames\n",
        "        self.crop_size = crop_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.z_dim = z_dim\n",
        "        self.conv_init = conv_init\n",
        "        self.beta1 = beta1\n",
        "        self.disc_iterations = disc_iterations\n",
        "        self.gen_iterations = gen_iterations\n",
        "        self.num_out = num_out\n",
        "        self.epochs = epochs\n",
        "        self.save_int = save_int\n",
        "        self.save_dir = save_dir\n",
        "        self.save_checkpts = save_checkpts\n",
        "                \n",
        "        self.generator = self.generator_model()\n",
        "        self.discriminator = self.discriminator_model()\n",
        "\n",
        "        self.gen_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1, beta2=0.999)\n",
        "        self.disc_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1, beta2=0.999)\n",
        "        \n",
        "        self.checkpoint = tf.train.Checkpoint(generator_optimizer=self.gen_optimizer,\n",
        "                                              discriminator_optimizer=self.disc_optimizer,\n",
        "                                              generator=self.generator,\n",
        "                                              discriminator=self.discriminator)\n",
        "\n",
        "    def train_step(self, videos):\n",
        "        # Generate noise from normal distribution\n",
        "        noise = tf.random_normal([self.batch_size, self.z_dim])\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            generated_videos = self.generator(noise, training=True)\n",
        "            \n",
        "            real_disc = self.discriminator(videos, training=True)\n",
        "            generated_disc = self.discriminator(generated_videos, training=True)\n",
        "            print('Disc scores real/fake:', tf.reduce_mean(real_disc), tf.reduce_mean(generated_disc))\n",
        "\n",
        "            gen_loss = self.generator_loss(generated_disc)\n",
        "            print('Gen loss:', gen_loss)\n",
        "            disc_loss = self.discriminator_loss(videos, generated_videos, real_disc, generated_disc)\n",
        "            print('Disc loss:', disc_loss)\n",
        "            \n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "        \n",
        "        for iter in range(self.gen_iterations):\n",
        "            self.gen_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "\n",
        "        for iter in range(self.disc_iterations):\n",
        "            self.disc_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))       \n",
        "       \n",
        "    def train(self):\n",
        "        # Plot model structure\n",
        "        tf.keras.utils.plot_model(self.generator, show_shapes=True, \n",
        "                                  to_file=os.path.join(self.save_dir, 'gen.jpg'))\n",
        "        tf.keras.utils.plot_model(self.discriminator, show_shapes=True,\n",
        "                                  to_file=os.path.join(self.save_dir, 'disc.jpg'))\n",
        "        \n",
        "        # Generate noise from normal distribution\n",
        "        for epoch in range(self.epochs):\n",
        "            start = time.time()\n",
        "\n",
        "            for batch in self.videos:\n",
        "                self.train_step(batch)\n",
        "\n",
        "            # Save every n intervals\n",
        "            if (epoch + 1) % self.save_int == 0:\n",
        "                self.generate(self.generator, epoch + 1, self.num_out)\n",
        "                if self.save_checkpts:\n",
        "                    self.checkpoint.save(file_prefix = os.path.join(VIDEO_DIR, \"ckpt\"))\n",
        "\n",
        "            print ('Time taken for epoch {} is {} sec'.format(epoch + 1,\n",
        "                                                              time.time()-start))\n",
        "        # Generate samples after final epoch\n",
        "        self.generate(self.generator, self.epochs, self.num_out)\n",
        "\n",
        "    def generator_model(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        \n",
        "        # Linear block\n",
        "        model.add(tf.keras.layers.Dense(self.crop_size*8 * 4 * 4 * 2, input_shape=(self.z_dim,),\n",
        "                                        kernel_initializer=tf.keras.initializers.random_normal(stddev=0.01)))\n",
        "        model.add(tf.keras.layers.Reshape((2, 4, 4, self.crop_size*8)))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.ReLU())\n",
        "        \n",
        "        # Convolution block 1\n",
        "        model.add(tf.keras.layers.Conv3DTranspose(filters=self.crop_size*4, kernel_size=4, strides=2, padding='same', \n",
        "                                                  kernel_initializer=self.conv_init, use_bias=True))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.ReLU())\n",
        "    \n",
        "        # Convolution block 2\n",
        "        model.add(tf.keras.layers.Conv3DTranspose(filters=self.crop_size*2, kernel_size=4, strides=2, padding='same', \n",
        "                                                  kernel_initializer=self.conv_init, use_bias=True))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.ReLU())\n",
        "    \n",
        "        # Convolution block 3\n",
        "        model.add(tf.keras.layers.Conv3DTranspose(filters=self.crop_size, kernel_size=4, strides=2, padding='same', \n",
        "                                                  kernel_initializer=self.conv_init, use_bias=True))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.ReLU())\n",
        "    \n",
        "        # Convolution block 4\n",
        "        model.add(tf.keras.layers.Conv3DTranspose(filters=3, kernel_size=4, strides=2, padding='same', \n",
        "                                                  kernel_initializer=self.conv_init, use_bias=True, activation='tanh'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def discriminator_model(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        \n",
        "        # Convolution block 1\n",
        "        model.add(tf.keras.layers.Conv3D(filters=self.crop_size, \n",
        "                                         input_shape=(self.num_frames, self.crop_size, self.crop_size, 3),\n",
        "                                         kernel_size=4, strides=2, padding='same', kernel_initializer=self.conv_init))\n",
        "        model.add(tf.keras.layers.Lambda(lambda x: tf.contrib.layers.layer_norm(x)))\n",
        "        model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "                  \n",
        "        # Convolution block 2\n",
        "        model.add(tf.keras.layers.Conv3D(filters=self.crop_size*2, kernel_size=4, strides=2, padding='same',\n",
        "                                         kernel_initializer=self.conv_init))\n",
        "        model.add(tf.keras.layers.Lambda(lambda x: tf.contrib.layers.layer_norm(x)))\n",
        "        model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "  \n",
        "        # Convolution block 3\n",
        "        model.add(tf.keras.layers.Conv3D(filters=self.crop_size*4, kernel_size=4, strides=2, padding='same',\n",
        "                                         kernel_initializer=self.conv_init))\n",
        "        model.add(tf.keras.layers.Lambda(lambda x: tf.contrib.layers.layer_norm(x)))\n",
        "        model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "        \n",
        "        # Convolution block 4\n",
        "        model.add(tf.keras.layers.Conv3D(filters=self.crop_size*8, kernel_size=4, strides=2, padding='same',\n",
        "                                         kernel_initializer=self.conv_init))\n",
        "        model.add(tf.keras.layers.Lambda(lambda x: tf.contrib.layers.layer_norm(x)))\n",
        "        model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "        \n",
        "        # Convolution block 5\n",
        "        model.add(tf.keras.layers.Conv3D(filters=1, kernel_size=4, strides=2, padding='same',\n",
        "                                         kernel_initializer=self.conv_init))\n",
        "        model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "                  \n",
        "        # Linear block\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.random_normal(stddev=0.01)))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def generator_loss(self, generated_disc):\n",
        "        # WGAN-GP loss: https://arxiv.org/pdf/1704.00028.pdf\n",
        "        # Negative so that gradient descent maximizes critic score received by generated output\n",
        "        return -tf.reduce_mean(generated_disc)\n",
        "\n",
        "    def discriminator_loss(self, real_videos, generated_videos, real_disc, generated_disc):\n",
        "        # WGAN-GP loss: https://arxiv.org/pdf/1704.00028.pdf\n",
        "        # Difference between critic scores received by generated output vs real video\n",
        "        # Lower values mean that the real video samples are receiving higher scores, therefore\n",
        "        # gradient descent maximizes discriminator accuracy\n",
        "        d_cost = tf.reduce_mean(generated_disc) - tf.reduce_mean(real_disc)\n",
        "        alpha = tf.random_uniform(\n",
        "            shape=[self.batch_size, 1],\n",
        "            minval=0.,\n",
        "            maxval=1.\n",
        "        )\n",
        "        dim = self.num_frames * self.crop_size * self.crop_size * 3\n",
        "        real = tf.reshape(real_videos, [self.batch_size, dim])\n",
        "        fake = tf.reshape(generated_videos, [self.batch_size, dim])\n",
        "        diff = fake - real\n",
        "        # Real videos adjusted by randomly weighted difference between real vs generated\n",
        "        interpolates = real + (alpha*diff)\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(interpolates)\n",
        "            interpolates_reshaped = tf.reshape(interpolates,\n",
        "                                   [self.batch_size, self.num_frames, \n",
        "                                    self.crop_size, self.crop_size, 3])\n",
        "            interpolates_disc = self.discriminator(interpolates_reshaped)\n",
        "        # Gradient of critic score wrt interpolated videos \n",
        "        gradients = tape.gradient(interpolates_disc, [interpolates])[0]\n",
        "        # Euclidean norm of gradient for each sample\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1]))\n",
        "        # Gradient norm penalty is the average distance from 1\n",
        "        gradient_penalty = tf.reduce_mean((norm - 1.) ** 2)\n",
        "        \n",
        "        return d_cost + 10 * gradient_penalty\n",
        "\n",
        "    def generate(self, model, epoch, num_out):\n",
        "        gen_noise = tf.random_normal([num_out, self.z_dim])\n",
        "        output = model(gen_noise, training=False)\n",
        "        frames = [convert_image(output[:, i, :, :, :], self.num_out) for i in range(self.num_frames)]\n",
        "        write_avi(frames, self.save_dir, name=str(epoch) + '.avi')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqSMGIizIvDF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train"
      ]
    },
    {
      "metadata": {
        "id": "ZAoVjA-7IyHE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VideoGAN(dataset,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 num_frames=FRAME_CAP,\n",
        "                 crop_size=VIDEO_SIZE[0],\n",
        "                 learning_rate=LEARNING_RATE,\n",
        "                 z_dim=Z_DIM,\n",
        "                 conv_init='he_normal',\n",
        "                 beta1=BETA1,\n",
        "                 disc_iterations=DISC_ITERATIONS,\n",
        "                 gen_iterations=GEN_ITERATIONS,\n",
        "                 epochs=EPOCHS,\n",
        "                 num_out=NUM_OUT,\n",
        "                 save_int=SAVE_RATE,\n",
        "                 save_dir=VIDEO_DIR)\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}