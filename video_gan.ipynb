{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video-gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6Ja32ubOA6n4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Video GAN\n",
        "\n",
        "This is both a replication exercise for learning and an experiment in novel artistic outputs. The goal is to generate abstracted videos evoking the subjective affect of certain objects, actions, and scenes in motion, through generative adversarial networks trained on input videos of the desired subjects.\n",
        "\n",
        "Generative models are based on [\"Generating Videos with Scene Dynamics\" (2016)](http://www.cs.columbia.edu/~vondrick/tinyvideo/paper.pdf) and [\"Improving Video Generation for Multi-functional Applications\" (2017)](https://arxiv.org/pdf/1711.11453.pdf). \n",
        "\n",
        "The creative part of this project is more nebulous for now but will require manipulating the generated videos such that they're able to be projected in a live setting paired with musical compositions. TBD...\n",
        "\n",
        "Using Google Colaboratory for TPU access. Will refactor once validated."
      ]
    },
    {
      "metadata": {
        "id": "8ayRtp_AWe-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eOnuODD1ArGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ]
    },
    {
      "metadata": {
        "id": "tOVBnAFniibX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Video settings\n",
        "video_dir = ''\n",
        "video_size = []\n",
        "frame_int = 2\n",
        "frame_cap = 32\n",
        "\n",
        "# Training parameters\n",
        "epochs = 50\n",
        "z_dim = 100\n",
        "read_threads = 16\n",
        "\n",
        "# Adam optimizer\n",
        "learning_rate = 0.0001\n",
        "beta1 = 0.5\n",
        "\n",
        "# Output frequency\n",
        "sample_rate = 100\n",
        "\n",
        "# Use eager execution\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PdUyvKGHSt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Video Processing"
      ]
    },
    {
      "metadata": {
        "id": "NX7WdeMsL9st",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract frames"
      ]
    },
    {
      "metadata": {
        "id": "fyesgxzfMu2R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "videos = glob.glob(os.path.join(video_dir, '*.avi'))\n",
        "\n",
        "# For each video in directory, capture every frame_int number of frames and store in 4D array.\n",
        "for vnum, video in enumerate(videos):\n",
        "  description = os.path.splitext(video)[0]\n",
        "  vidcap = cv2.VideoCapture(os.path.join(video_dir, video))\n",
        "  success, image = vidcap.read()\n",
        "  output = np.zeros(frame_cap, image.shape[0], image.shape[1], image.shape[2])\n",
        "  loc, frames = 0\n",
        "  while success and frames < frame_cap:\n",
        "    output[frames] = image\n",
        "    loc += frame_int\n",
        "    frames += 1\n",
        "    vidcap.set(cv2.CAP_PROP_POS_MSEC, count)\n",
        "    success, image = vidcap.read()\n",
        "  cv2.imwrite(os.path.join(video_dir, description + str(vnum) + '.jpg', output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIWfnu_ML_3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Read frames into tf data object"
      ]
    },
    {
      "metadata": {
        "id": "7pOYKYozHWuL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reads video image, decodes into a dense tensor, resized to desired shape.\n",
        "def _parse_function(filename, label):\n",
        "  image_string = tf.read_file(filename)\n",
        "  image_decoded = tf.image.decode_jpeg(image_string)\n",
        "  image_resized = tf.image.resize_images(image_decoded, video_size)\n",
        "  return image_resized, label\n",
        "\n",
        "# File name vector.\n",
        "video_files = glob.glob(os.path.join(video_dir, '*.jpg'))\n",
        "filenames = tf.constant(video_files)\n",
        "\n",
        "# Label vector.\n",
        "labels = tf.constant([os.path.splitext(vid)[0] for vid in video_files])\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "dataset = dataset.map(_parse_function)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifvlQ0T6A2YG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ]
    },
    {
      "metadata": {
        "id": "JMYU7WTlA3Xw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxRCYsGWA0Qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "GYqdBq3biAqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VideoGAN():\n",
        "  \n",
        "  def __init___(self):\n",
        "    \n",
        "    \n",
        "  def build_model(self):\n",
        "    \n",
        "  def train(self):\n",
        "    \n",
        "  def generator(self):\n",
        "    \n",
        "  def discriminator(self):"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}