{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video-gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6Ja32ubOA6n4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Video GAN\n",
        "\n",
        "This is a replication exercise for learning, an experiment in novel artistic outputs, and (hopefully, eventually) a research contribution to GANs as applied to video synthesis. The goal is to generate abstracted videos evoking the subjective affect of certain objects, actions, and scenes in motion, through generative adversarial networks trained on input videos of the desired subjects.\n",
        "\n",
        "Generative models are based on [\"Generating Videos with Scene Dynamics\" (2016)](http://www.cs.columbia.edu/~vondrick/tinyvideo/paper.pdf) and [\"Improving Video Generation for Multi-functional Applications\" (2017)](https://arxiv.org/pdf/1711.11453.pdf). Code is based on the latter's [GitHub repo](https://github.com/bernhard2202/improved-video-gan/) with updates for deprecated functions and eventually adjustments for intended outputs.\n",
        "\n",
        "The creative part of this project is more nebulous for now but will require manipulating the generated videos such that they're able to be projected in a live setting paired with musical compositions. At minimum this will require interpolating the outputs which will be pretty small, or figuring out a way to generate larger outputs without significant runtime cost.\n",
        "\n",
        "Using Google Colaboratory for TPU access. Will refactor into Python module once validated."
      ]
    },
    {
      "metadata": {
        "id": "8ayRtp_AWe-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8bosNVk3cdFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eOnuODD1ArGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ]
    },
    {
      "metadata": {
        "id": "tOVBnAFniibX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Video read settings\n",
        "VIDEO_DIR = '/content/drive/My Drive/Colab Data/video-gan'\n",
        "VIDEO_SIZE = [64, 64]\n",
        "INPUT_SIZE = [240, 320]\n",
        "FRAME_INT = 1\n",
        "FRAME_CAP = 32\n",
        "\n",
        "# Training parameters\n",
        "BUFFER_SIZE = 100000\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 50\n",
        "Z_DIM = 100\n",
        "CRIT_ITERATIONS = 5\n",
        "\n",
        "# Adam optimizer\n",
        "LEARNING_RATE = 0.0001\n",
        "BETA1 = 0.5\n",
        "\n",
        "# Output frequency\n",
        "SAMPLE_RATE = 100\n",
        "SAVE_RATE = 100\n",
        "NUM_OUT = 1\n",
        "\n",
        "# Use eager execution\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PdUyvKGHSt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Video Processing"
      ]
    },
    {
      "metadata": {
        "id": "NX7WdeMsL9st",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract frames"
      ]
    },
    {
      "metadata": {
        "id": "fyesgxzfMu2R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "videos = glob.glob(os.path.join(VIDEO_DIR, '*.avi'))\n",
        "\n",
        "# For each video in directory, capture every frame_int number of frames and \n",
        "# store in array where each frame is stacked horizontally.\n",
        "for vnum, video in enumerate(videos):\n",
        "    description = os.path.splitext(video)[0]\n",
        "    vidcap = cv2.VideoCapture(os.path.join(VIDEO_DIR, video))\n",
        "    success, image = vidcap.read()\n",
        "    output = np.zeros((FRAME_CAP * image.shape[0], image.shape[1], image.shape[2]))\n",
        "    loc, frames = 0, 0\n",
        "    while success and frames < FRAME_CAP:\n",
        "        output[frames * image.shape[0]:(frames + 1) * image.shape[0]] = image\n",
        "        loc += FRAME_INT\n",
        "        frames += 1\n",
        "        vidcap.set(cv2.CAP_PROP_POS_MSEC, loc)\n",
        "        success, image = vidcap.read()\n",
        "    input_size = image.shape[:2]\n",
        "    cv2.imwrite(os.path.join(VIDEO_DIR, description + str(vnum) + '.jpg'), np.float32(output))\n",
        "vidcap.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIWfnu_ML_3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Read frames into tf data object"
      ]
    },
    {
      "metadata": {
        "id": "7pOYKYozHWuL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reads video image, decodes into tensor, resized to desired shape.\n",
        "def parse_video(filename):\n",
        "    image_string = tf.read_file(filename)\n",
        "    image_decoded = tf.cast(tf.image.decode_jpeg(image_string, channels=3), tf.float32)\n",
        "    frames = tf.reshape(image_decoded, [-1, input_size[0], input_size[1], 3])\n",
        "    image_resized = tf.image.resize_images(frames, VIDEO_SIZE)\n",
        "    return tf.subtract(tf.math.divide(image_resized, 127.5), 1.0)\n",
        "\n",
        "# File name vector.\n",
        "all_image_paths = glob.glob(os.path.join(VIDEO_DIR, '*.jpg'))\n",
        "all_image_paths = [str(path) for path in all_image_paths]\n",
        "\n",
        "# Construct dataset, shuffle, and repeat.\n",
        "# TODO: Try shuffling after mapping?\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_image_paths).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.map(parse_video).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifvlQ0T6A2YG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ]
    },
    {
      "metadata": {
        "id": "Zojuu-udqeCs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Custom Layers"
      ]
    },
    {
      "metadata": {
        "id": "NKPQ7IclqeRV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DiscriminatorLayer(tf.keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size, strides, normalize=False):\n",
        "        super(DiscriminatorLayer, self).__init__(name='')\n",
        "        self.filters = input_dim * output_dim * kernel_size ** 3\n",
        "        self.k = kernel_size\n",
        "        self.strides = strides\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        out = tf.keras.layers.Conv3D(filters=self.filters, \n",
        "                                     kernel_size=self.k,\n",
        "                                     strides=self.strides,\n",
        "                                     padding='same',\n",
        "                                     kernel_initializer='he_normal')(input_tensor)\n",
        "        if self.normalize:\n",
        "            out = tf.contrib.layers.layer_norm(out)\n",
        "            \n",
        "        return tf.nn.leaky_relu(out)\n",
        "    \n",
        "class LinearLayer(tf.keras.Model):\n",
        "    def __init__(self, output_dim, resize_in=None, resize_out=None):\n",
        "        super(LinearLayer, self).__init__(name='')\n",
        "        self.out_dim = output_dim\n",
        "        self.resize_in = resize_in\n",
        "        self.resize_out = resize_out\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        if self.resize_in is not None:\n",
        "            input_tensor = tf.reshape(input_tensor, self.resize_in)\n",
        "        shape = input_tensor.shape\n",
        "        matrix = tf.random.normal([shape[1], self.out_dim], stddev=0.01)\n",
        "        out = tf.matmul(input_tensor, matrix)\n",
        "        if self.resize_out is not None:\n",
        "            out = tf.reshape(out, self.resize_out)\n",
        "        \n",
        "        return out\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KAVQVC91TRR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Output Utilities"
      ]
    },
    {
      "metadata": {
        "id": "C7hiV9hzTK2Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_avi(batch, directory, name=''):\n",
        "    writer = cv2.VideoWriter(out, cv2.VideoWriter_fourcc('X', 'V', 'I', 'D'),\n",
        "                             frate, (video_size[0], video_size[1]))\n",
        "    for fnum, frame in enumerate(batch):\n",
        "        writer.write(np.uint8(frame))\n",
        "    writer.release()\n",
        "\n",
        "    \n",
        "def convert_image(images, batch_size, col=5, row=5):\n",
        "    images = tf.image.convert_image_dtype(tf.div(tf.add(images, 1.0), 2.0), tf.uint8)\n",
        "    images = [image for image in tf.split(images, batch_size, axis=0)]\n",
        "    rows = []\n",
        "    for i in range(row):\n",
        "        rows.append(tf.concat(images[col * i + 0:col * i + col], 2))\n",
        "    image = tf.concat(rows, 1)\n",
        "    return tf.image.encode_jpeg(tf.squeeze(image, [0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxRCYsGWA0Qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "GYqdBq3biAqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VideoGAN():\n",
        "  \n",
        "    def __init__(self,\n",
        "                 input,\n",
        "                 batch_size,\n",
        "                 frame_size,\n",
        "                 crop_size,\n",
        "                 learning_rate,\n",
        "                 z_dim,\n",
        "                 beta1,\n",
        "                 critical_iterations,\n",
        "                 num_out,\n",
        "                 epochs):\n",
        "        self.critical_iterations = critical_iterations\n",
        "        self.crop_size = crop_size\n",
        "        self.beta1 = beta1\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.z_dim = z_dim\n",
        "        self.frame_size = frame_size\n",
        "        self.videos = input\n",
        "        self.num_out = num_out\n",
        "        self.epochs = epochs\n",
        "        \n",
        "        self.generator = self.generator_model()\n",
        "        self.discriminator = self.discriminator_model()\n",
        "\n",
        "        self.gen_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1, beta2=0.999)\n",
        "        self.disc_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1, beta2=0.999)\n",
        "\n",
        "    def train_step(self, videos):\n",
        "        # Generate noise from normal distribution\n",
        "        noise = tf.random_normal([self.batch_size, self.z_dim])\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            generated_videos = self.generator(noise, training=True)\n",
        "\n",
        "            real_output = self.discriminator(videos, training=True)\n",
        "            generated_output = self.discriminator(generated_videos, training=True)\n",
        "\n",
        "            gen_loss = self.generator_loss(generated_output)\n",
        "            disc_loss = self.discriminator_loss(real_output, generated_output)\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.variables)\n",
        "\n",
        "        self.gen_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.variables))\n",
        "        self.disc_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.variables))        \n",
        "       \n",
        "    def train(self):\n",
        "        # Generate noise from normal distribution\n",
        "        random_vector_for_generation = tf.random_normal([self.num_out, self.z_dim])\n",
        "        for epoch in range(self.epochs):\n",
        "            start = time.time()\n",
        "\n",
        "            for video in self.videos:\n",
        "                self.train_step(video)\n",
        "\n",
        "            display.clear_output(wait=True)\n",
        "            self.generate_and_save(self.generator,\n",
        "                                   epoch + 1,\n",
        "                                   random_vector_for_generation)\n",
        "\n",
        "            # Save every n intervals\n",
        "            if (epoch + 1) % self.save_int == 0:\n",
        "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "            print ('Time taken for epoch {} is {} sec'.format(epoch + 1,\n",
        "                                                              time.time()-start))\n",
        "        # Generate samples after final epoch\n",
        "        display.clear_output(wait=True)\n",
        "        self.generate_and_save(generator,\n",
        "                               self.epochs,\n",
        "                               random_vector_for_generation)\n",
        "\n",
        "    def generator_model(self):\n",
        "        model = tf.keras.Sequential()\n",
        "        \n",
        "        # Linear block\n",
        "        model.add(LinearLayer(resize_out=[-1, 2, 4, 4, 512], output_dim=512 * 4 * 4 * 2))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "        \n",
        "        # Convolution block 1\n",
        "        model.add(tf.keras.layers.Conv3DTranspose(filters=512, kernel_size=4, strides=2, padding='same', \n",
        "                                                  kernel_initializer='he_normal', use_bias=True))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "        \n",
        "        # Convolution block 2\n",
        "        model.add(tf.keras.layers.Conv3DTranspose(filters=256, kernel_size=4, strides=2, padding='same', \n",
        "                                                  kernel_initializer='he_normal', use_bias=True))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "        \n",
        "        # Convolution block 3\n",
        "        model.add(tf.keras.layers.Conv3DTranspose(filters=128, kernel_size=4, strides=2, padding='same', \n",
        "                                                  kernel_initializer='he_normal', use_bias=True))\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.LeakyReLU())\n",
        "        \n",
        "        # Convolution block 4\n",
        "        model.add(tf.keras.layers.Conv3DTranspose(filters=64, kernel_size=4, strides=2, padding='same', \n",
        "                                                  kernel_initializer='he_normal', use_bias=True, activation='tanh'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def discriminator_model(self):\n",
        "        initial_dim = self.crop_size\n",
        "        model = tf.keras.Sequential()\n",
        "        \n",
        "        # Convolution block 1\n",
        "        model.add(DiscriminatorLayer(input_dim=3, output_dim=initial_dim, kernel_size=4, strides=2))\n",
        "                  \n",
        "        # Convolution block 2\n",
        "        model.add(DiscriminatorLayer(input_dim=initial_dim, output_dim=initial_dim * 2, kernel_size=4, strides=2))\n",
        "                  \n",
        "        # Convolution block 3\n",
        "        model.add(DiscriminatorLayer(input_dim=initial_dim * 2, output_dim=initial_dim * 4, kernel_size=4, strides=2))\n",
        "        \n",
        "        # Convolution block 4\n",
        "        model.add(DiscriminatorLayer(input_dim=initial_dim * 4, output_dim=initial_dim * 8, kernel_size=4, strides=2))\n",
        "        \n",
        "        # Convolution block 5\n",
        "        model.add(DiscriminatorLayer(input_dim=initial_dim * 8, output_dim=1, kernel_size=4, strides=2, normalize=False))\n",
        "                  \n",
        "        # Linear block\n",
        "        model.add(LinearLayer(resize_in=[self.batch_size, -1], output_dim=1))\n",
        "                  \n",
        "        return model\n",
        "\n",
        "    def generator_loss(self, generated_output):\n",
        "        return -tf.reduce_mean(generated_output)\n",
        "\n",
        "    def discriminator_loss(self, real_output, generated_output):\n",
        "        # Discriminator uses Wasserstein earth-mover loss function\n",
        "        d_cost = tf.reduce_mean(generated_output) - tf.reduce_mean(real_output)\n",
        "        alpha = tf.random_uniform(\n",
        "            shape=[self.batch_size, 1],\n",
        "            minval=0.,\n",
        "            maxval=1.\n",
        "        )\n",
        "        dim = self.frame_size * self.crop_size * self.crop_size * 3\n",
        "        real = tf.reshape(self.videos, [self.batch_size, dim])\n",
        "        fake = tf.reshape(generated_output, [self.batch_size, dim])\n",
        "        diff = fake - real\n",
        "        interpolates = real + (alpha * diff)\n",
        "        interpolates_reshaped = tf.reshape(interpolates, \n",
        "                                           [self.batch_size, self.frame_size, self.crop_size, self.crop_size, 3])\n",
        "        with tf.GradientTape() as tape:\n",
        "            d_hat, _ = self.discriminator(interpolates_reshaped, reuse=True)\n",
        "        gradients = tape.gradient(d_hat, interpolates)[0]\n",
        "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
        "        gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
        "        \n",
        "        return d_cost + 10 * gradient_penalty\n",
        "\n",
        "    def generate_and_save(self, model, epoch, test_input):\n",
        "        # TODO: This is for images not videos right now\n",
        "        # Make sure the training parameter is set to False because we\n",
        "        # don't want to train the batchnorm layer when doing inference.\n",
        "        predictions = model(test_input, training=False)\n",
        "\n",
        "        fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "        for i in range(predictions.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqSMGIizIvDF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train"
      ]
    },
    {
      "metadata": {
        "id": "ZAoVjA-7IyHE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VideoGAN(dataset,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 frame_size=FRAME_CAP,\n",
        "                 crop_size=VIDEO_SIZE[0],\n",
        "                 learning_rate=LEARNING_RATE,\n",
        "                 z_dim=Z_DIM,\n",
        "                 beta1=BETA1,\n",
        "                 critical_iterations=CRIT_ITERATIONS,\n",
        "                 epochs=EPOCHS,\n",
        "                 num_out=NUM_OUT)\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}