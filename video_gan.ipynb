{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video-gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "6Ja32ubOA6n4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Video GAN\n",
        "\n",
        "This is a replication exercise for learning, an experiment in novel artistic outputs, and (hopefully, eventually) a research contribution to GANs as applied to video synthesis. The goal is to generate abstracted videos evoking the subjective affect of certain objects, actions, and scenes in motion, through generative adversarial networks trained on input videos of the desired subjects.\n",
        "\n",
        "Generative models are based on [\"Generating Videos with Scene Dynamics\" (2016)](http://www.cs.columbia.edu/~vondrick/tinyvideo/paper.pdf) and [\"Improving Video Generation for Multi-functional Applications\" (2017)](https://arxiv.org/pdf/1711.11453.pdf). Code is based on the latter's [GitHub repo](https://github.com/bernhard2202/improved-video-gan/). Kratzwald's implementation appears to be a better fit for desired use case due to its ability to handle inputs without static backgrounds.\n",
        "\n",
        "The creative part of this project is more nebulous for now but will require manipulating the generated videos such that they're able to be projected in a live setting paired with musical compositions. At minimum this will require interpolating the outputs which will be pretty small, or figuring out a way to generate larger outputs without significant runtime cost.\n",
        "\n",
        "Using Google Colaboratory for TPU access. Will refactor into Python module once validated."
      ]
    },
    {
      "metadata": {
        "id": "8ayRtp_AWe-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eOnuODD1ArGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ]
    },
    {
      "metadata": {
        "id": "tOVBnAFniibX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Video settings\n",
        "video_dir = ''\n",
        "video_size = []\n",
        "frame_int = 2\n",
        "frame_cap = 32\n",
        "\n",
        "# Training parameters\n",
        "epochs = 50\n",
        "z_dim = 100\n",
        "read_threads = 16\n",
        "batch_size = 64\n",
        "\n",
        "# Adam optimizer\n",
        "learning_rate = 0.0001\n",
        "alpha1 = 0.1\n",
        "beta1 = 0.5\n",
        "\n",
        "# Output frequency\n",
        "sample_rate = 100\n",
        "\n",
        "# Use eager execution\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7PdUyvKGHSt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Video Processing"
      ]
    },
    {
      "metadata": {
        "id": "NX7WdeMsL9st",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract frames"
      ]
    },
    {
      "metadata": {
        "id": "fyesgxzfMu2R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "videos = glob.glob(os.path.join(video_dir, '*.avi'))\n",
        "\n",
        "# For each video in directory, capture every frame_int number of frames and store in 4D array.\n",
        "for vnum, video in enumerate(videos):\n",
        "    description = os.path.splitext(video)[0]\n",
        "    vidcap = cv2.VideoCapture(os.path.join(video_dir, video))\n",
        "    success, image = vidcap.read()\n",
        "    output = np.zeros(frame_cap, image.shape[0], image.shape[1], image.shape[2])\n",
        "    loc, frames = 0\n",
        "    while success and frames < frame_cap:\n",
        "        output[frames] = image\n",
        "        loc += frame_int\n",
        "        frames += 1\n",
        "        vidcap.set(cv2.CAP_PROP_POS_MSEC, count)\n",
        "        success, image = vidcap.read()\n",
        "    cv2.imwrite(os.path.join(video_dir, description + str(vnum) + '.jpg', output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIWfnu_ML_3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Read frames into tf data object"
      ]
    },
    {
      "metadata": {
        "id": "7pOYKYozHWuL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reads video image, decodes into a dense tensor, resized to desired shape.\n",
        "def _parse_function(filename, label):\n",
        "    image_string = tf.read_file(filename)\n",
        "    image_decoded = tf.image.decode_jpeg(image_string)\n",
        "    image_resized = tf.image.resize_images(image_decoded, video_size)\n",
        "    return image_resized, label\n",
        "\n",
        "# File name vector.\n",
        "video_files = glob.glob(os.path.join(video_dir, '*.jpg'))\n",
        "filenames = tf.constant(video_files)\n",
        "\n",
        "# Label vector.\n",
        "labels = tf.constant([os.path.splitext(vid)[0] for vid in video_files])\n",
        "\n",
        "# Construct dataset, shuffle, and repeat.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels)).shuffle(buffer_size=10000).repeat(epochs)\n",
        "dataset = dataset.map(_parse_function).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifvlQ0T6A2YG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utilities\n",
        "\n",
        "### Model Utilities"
      ]
    },
    {
      "metadata": {
        "id": "JMYU7WTlA3Xw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model summaries\n",
        "\n",
        "def add_activation_summary(var):\n",
        "    tf.summary.histogram(var.op.name + \"/activation\", var)\n",
        "    tf.summary.scalar(var.op.name + \"/sparsity\", tf.nn.zero_fraction(var))\n",
        "\n",
        "def add_gradient_summary(grad, var):\n",
        "    if grad is not None:\n",
        "        tf.summary.histogram(var.op.name + '/gradient', grad)\n",
        "        \n",
        "# Layer utilities\n",
        "\n",
        "def uniform(std_dev, size):\n",
        "    return np.random.uniform(\n",
        "        low=-std_dev * np.sqrt(3),\n",
        "        high=std_dev * np.sqrt(3),\n",
        "        size=size\n",
        "    ).astype('float32')\n",
        "    \n",
        "def conv2d(input_, input_dim, output_dim,\n",
        "           k_h=4, k_w=4, d_h=2, d_w=2, name=\"conv2d\", padding=\"SAME\"):\n",
        "    \"\"\" \n",
        "    init weights like in    \n",
        "    \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\"\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "        fan_in = input_dim * k_h * k_w\n",
        "        fan_out = (output_dim * k_h * k_w) / (d_h * d_w)\n",
        "\n",
        "        filters_std = np.sqrt(4. / (fan_in + fan_out))\n",
        "\n",
        "        filter_values = uniform(\n",
        "            filters_std,\n",
        "            (k_h, k_w, input_dim, output_dim)\n",
        "        )\n",
        "\n",
        "        w_init = tf.Variable(filter_values, name='filters_init')\n",
        "        w = tf.get_variable('filters', initializer=w_init.initialized_value())\n",
        "        b = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "        result = tf.nn.conv2d(\n",
        "            input=input_,\n",
        "            filter=w,\n",
        "            strides=[1, d_h, d_w, 1],\n",
        "            padding=padding,\n",
        "            data_format='NHWC'\n",
        "        )\n",
        "        result = tf.nn.bias_add(result, b)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def conv3d(input_, input_dim, output_dim,\n",
        "           k_t=4, k_h=4, k_w=4, d_t=2, d_h=2, d_w=2, name=\"conv3d\", padding=\"SAME\"):\n",
        "    \"\"\" \n",
        "    init weights like in \n",
        "    \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\"\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "        fan_in = input_dim * k_t * k_h * k_w\n",
        "        fan_out = (output_dim * k_t * k_h * k_w) / (d_t * d_h * d_w)\n",
        "\n",
        "        filters_std = np.sqrt(4. / (fan_in + fan_out))\n",
        "\n",
        "        filter_values = uniform(\n",
        "            filters_std,\n",
        "            (k_t, k_h, k_w, input_dim, output_dim)\n",
        "        )\n",
        "\n",
        "        w_init = tf.Variable(filter_values, name='filters_init')\n",
        "        w = tf.get_variable('filters', initializer=w_init.initialized_value())\n",
        "        b = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        result = tf.nn.conv3d(\n",
        "            input=input_,\n",
        "            filter=w,\n",
        "            strides=[1, d_t, d_h, d_w, 1],\n",
        "            padding=padding,\n",
        "            data_format='NDHWC'\n",
        "        )\n",
        "        result = tf.nn.bias_add(result, b)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def conv3d_transpose(input_, input_dim, output_shape,\n",
        "                     k_h=4, k_w=4, k_d=4, d_h=2, d_w=2, d_d=2,\n",
        "                     name=\"deconv3d\"):\n",
        "    \"\"\" \n",
        "    init weights like in \n",
        "    \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\"\n",
        "    \"\"\"\n",
        "    with tf.variable_scope(name):\n",
        "        output_dim = output_shape[-1]\n",
        "\n",
        "        fan_in = input_dim * k_d * k_h * k_w\n",
        "        fan_out = (output_dim * k_d * k_h * k_w) / (d_d * d_h * d_w)\n",
        "\n",
        "        filters_std = np.sqrt(4. / (fan_in + fan_out))\n",
        "\n",
        "        filter_values = uniform(\n",
        "            filters_std,\n",
        "            (k_d, k_h, k_w, output_dim, input_dim)\n",
        "        )\n",
        "\n",
        "        w_init = tf.Variable(filter_values, name='filter_init')\n",
        "        w = tf.get_variable('filters', initializer=w_init.initialized_value())\n",
        "        b = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        result = tf.nn.conv3d_transpose(value=input_,\n",
        "                                        filter=w,\n",
        "                                        output_shape=output_shape,\n",
        "                                        strides=[1, d_d, d_h, d_w, 1],\n",
        "                                        name=name,\n",
        "                                        )\n",
        "\n",
        "        result = tf.nn.bias_add(result, b)\n",
        "        return result\n",
        "\n",
        "    \n",
        "def dis_block(input, input_dim, output_dim, name, reuse=False, normalize=True):\n",
        "    with tf.variable_scope(name, reuse=reuse) as vs:\n",
        "        result = conv3d(input, input_dim, output_dim, name='conv3d')\n",
        "        if normalize:\n",
        "            result = tf.contrib.layers.layer_norm(result, reuse=reuse, scope=vs)\n",
        "        result = tf.maximum(result, result * 0.2)\n",
        "    return result\n",
        "\n",
        "\n",
        "def linear(input_, output_size, scope=None, stddev=0.01, bias_start=0.0, with_w=False):\n",
        "    \"\"\"\n",
        "    Code from https://github.com/wxh1996/VideoGAN-tensorflow\n",
        "    \"\"\"\n",
        "    shape = input_.get_shape().as_list()\n",
        "    with tf.variable_scope(scope or \"Linear\"):\n",
        "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
        "                                 tf.random_normal_initializer(stddev=stddev))\n",
        "        bias = tf.get_variable(\"bias\", [output_size],\n",
        "                               initializer=tf.constant_initializer(bias_start))\n",
        "        if with_w:\n",
        "            return tf.matmul(input_, matrix) + bias, matrix, bias\n",
        "        else:\n",
        "            return tf.matmul(input_, matrix) + bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KAVQVC91TRR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Output Utilities"
      ]
    },
    {
      "metadata": {
        "id": "C7hiV9hzTK2Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_avi(batch, directory, name=''):\n",
        "    writer = cv2.VideoWriter(out, cv2.VideoWriter_fourcc('X', 'V', 'I', 'D'),\n",
        "                             frate, (dims[1], dims[0])) #TODO: Set dimension reference\n",
        "    for fnum, frame in enumerate(batch):\n",
        "        writer.write(np.uint8(frame))\n",
        "    writer.release()\n",
        "\n",
        "    \n",
        "def convert_image(images, batch_size, col=5, row=5):\n",
        "    images = tf.image.convert_image_dtype(tf.div(tf.add(images, 1.0), 2.0), tf.uint8)\n",
        "    images = [image for image in tf.split(images, batch_size, axis=0)]\n",
        "    rows = []\n",
        "    for i in range(row):\n",
        "        rows.append(tf.concat(images[col * i + 0:col * i + col], 2))\n",
        "    image = tf.concat(rows, 1)\n",
        "    return tf.image.encode_jpeg(tf.squeeze(image, [0]))\n",
        "\n",
        "\n",
        "def sampleBatch(samples, batch_size, col=5, row=5, frames=32):\n",
        "    frames = [convert_image(samples[:, i, :, :, :], batch_size, col, row) for i in range(frames)]\n",
        "    return frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VxRCYsGWA0Qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "GYqdBq3biAqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VideoGAN():\n",
        "  \n",
        "    def __init___(self,\n",
        "                input\n",
        "                batch_size,\n",
        "                frame_size,\n",
        "                crop_size,\n",
        "                learning_rate,\n",
        "                beta1,\n",
        "                critical_iterations):\n",
        "        self.critic_iterations = critic_iterations\n",
        "        self.crop_size = crop_size\n",
        "        self.beta1 = beta1\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.z_dim = z_dim\n",
        "        self.frame_size = frame_size\n",
        "        self.videos = input_batch\n",
        "        self.build_model()\n",
        "    \n",
        "    def build_model(self):\n",
        "        print(\"Setting up model...\")\n",
        "        self.z_vec = tf.placeholder(tf.float32, [self.batch_size, self.z_dim], name=\"z\")\n",
        "\n",
        "        tf.summary.histogram(\"z\", self.z_vec)\n",
        "        self.videos_fake, self.generator_variables = self.generator(self.z_vec)\n",
        "\n",
        "        self.d_real, self.discriminator_variables = self.discriminator(self.videos, reuse=False)\n",
        "        self.d_fake, _ = self.discriminator(self.videos_fake, reuse=True)\n",
        "\n",
        "        self.g_cost = -tf.reduce_mean(self.d_fake)\n",
        "        self.d_cost = tf.reduce_mean(self.d_fake) - tf.reduce_mean(self.d_real)\n",
        "\n",
        "        tf.summary.scalar(\"g_cost\", self.g_cost)\n",
        "        tf.summary.scalar(\"d_cost\", self.d_cost)\n",
        "\n",
        "        alpha = tf.random_uniform(\n",
        "            shape=[self.batch_size, 1],\n",
        "            minval=0.,\n",
        "            maxval=1.\n",
        "        )\n",
        "\n",
        "        dim = self.frame_size * self.crop_size * self.crop_size * 3\n",
        "\n",
        "        vid = tf.reshape(self.videos, [self.batch_size, dim])\n",
        "        fake = tf.reshape(self.videos_fake, [self.batch_size, dim])\n",
        "        differences = fake - vid\n",
        "        interpolates = vid + (alpha * differences)\n",
        "        d_hat, _ = self.discriminator(\n",
        "            tf.reshape(interpolates, [self.batch_size, self.frame_size, self.crop_size, self.crop_size, 3]), reuse=True)\n",
        "        gradients = tf.gradients(d_hat, [interpolates])[0]\n",
        "        slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
        "        gradient_penalty = tf.reduce_mean((slopes - 1.) ** 2)\n",
        "\n",
        "        self.d_cost_final = self.d_cost + 10 * gradient_penalty\n",
        "\n",
        "        tf.summary.scalar(\"d_cost_penalized\", self.d_cost_final)\n",
        "\n",
        "        self.d_adam, self.g_adam = None, None\n",
        "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "            self.d_adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1, beta2=0.999) \\\n",
        "                .minimize(self.d_cost_final, var_list=self.discriminator_variables)\n",
        "            self.g_adam = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.beta1, beta2=0.999) \\\n",
        "                .minimize(self.g_cost, var_list=self.generator_variables)\n",
        "\n",
        "        print(\"\\nTrainable variables for generator:\")\n",
        "        for var in self.generator_variables:\n",
        "            print(var.name)\n",
        "        print(\"\\nTrainable variables for discriminator:\")\n",
        "        for var in self.discriminator_variables:\n",
        "            print(var.name)\n",
        "\n",
        "        self.sample = sampleBatch(self.videos_fake, self.batch_size)\n",
        "        self.summary_op = tf.summary.merge_all()\n",
        "    \n",
        "    def train(self\n",
        "              session,\n",
        "              step,\n",
        "              summary_writer=None,\n",
        "              log_summary=False,\n",
        "              sample_dir=None,\n",
        "              generate_sample=False):\n",
        "        if log_summary:\n",
        "            start_time = time.time()\n",
        "\n",
        "        critic_itrs = self.critic_iterations\n",
        "\n",
        "        for critic_itr in range(critic_itrs):\n",
        "            session.run(self.d_adam, feed_dict=self.get_feed_dict())\n",
        "\n",
        "        feed_dict = self.get_feed_dict()\n",
        "        session.run(self.g_adam, feed_dict=feed_dict)\n",
        "\n",
        "        if log_summary:\n",
        "            g_loss_val, d_loss_val, summary = session.run([self.g_cost, self.d_cost_final, self.summary_op],\n",
        "                                                          feed_dict=feed_dict)\n",
        "            summary_writer.add_summary(summary, step)\n",
        "            print(\"Time: %g/itr, Step: %d, generator loss: %g, discriminator_loss: %g\" % (\n",
        "                time.time() - start_time, step, g_loss_val, d_loss_val))\n",
        "\n",
        "        if generate_sample:\n",
        "            vid_sample = session.run(self.sample, feed_dict=feed_dict)\n",
        "            saveGIFBatch(vid_sample, sample_dir, 'vid_%d' % step)\n",
        "            \n",
        "    def generator(self):\n",
        "        with tf.variable_scope('g_') as vs:\n",
        "            \"\"\" LINEAR BLOCK \"\"\"\n",
        "            self.z_, _, _ = linear(z, 512 * 4 * 4 * 2, 'g_f_h0_lin', with_w=True)\n",
        "            self.fg_h0 = tf.reshape(self.z_, [-1, 2, 4, 4, 512])\n",
        "            self.fg_h0 = tf.nn.relu(tf.contrib.layers.batch_norm(self.fg_h0, scope='g_f_bn0'), name='g_f_relu0')\n",
        "            add_activation_summary(self.fg_h0)\n",
        "\n",
        "            \"\"\" CONV BLOCK 1 \"\"\"\n",
        "            self.fg_h1 = conv3d_transpose(self.fg_h0, 512, [self.batch_size, 4, 8, 8, 256], name='g_f_h1')\n",
        "            self.fg_h1 = tf.nn.relu(tf.contrib.layers.batch_norm(self.fg_h1, scope='g_f_bn1'), name='g_f_relu1')\n",
        "            add_activation_summary(self.fg_h1)\n",
        "\n",
        "            \"\"\" CONV BLOCK 2 \"\"\"\n",
        "            self.fg_h2 = conv3d_transpose(self.fg_h1, 256, [self.batch_size, 8, 16, 16, 128], name='g_f_h2')\n",
        "            self.fg_h2 = tf.nn.relu(tf.contrib.layers.batch_norm(self.fg_h2, scope='g_f_bn2'), name='g_f_relu2')\n",
        "            add_activation_summary(self.fg_h2)\n",
        "\n",
        "            \"\"\" CONV BLOCK 3 \"\"\"\n",
        "            self.fg_h3 = conv3d_transpose(self.fg_h2, 128, [self.batch_size, 16, 32, 32, 64], name='g_f_h3')\n",
        "            self.fg_h3 = tf.nn.relu(tf.contrib.layers.batch_norm(self.fg_h3, scope='g_f_bn3'), name='g_f_relu3')\n",
        "            add_activation_summary(self.fg_h3)\n",
        "\n",
        "            \"\"\" CONV BLOCK 5 \"\"\"\n",
        "            self.fg_h4 = conv3d_transpose(self.fg_h3, 64, [self.batch_size, 32, 64, 64, 3], name='g_f_h4')\n",
        "            self.fg_fg = tf.nn.tanh(self.fg_h4, name='g_f_actvcation')\n",
        "\n",
        "        variables = tf.contrib.framework.get_variables(vs)\n",
        "        return self.fg_fg, variables\n",
        "    \n",
        "    def discriminator(self):\n",
        "        with tf.variable_scope('d_', reuse=reuse) as vs:\n",
        "            initial_dim = 64\n",
        "            \"\"\" CONV BLOCK 1 \"\"\"\n",
        "            d_h0 = dis_block(video, 3, initial_dim, 'block1', reuse=reuse)\n",
        "            \"\"\" CONV BLOCK 2 \"\"\"\n",
        "            d_h1 = dis_block(d_h0, initial_dim, initial_dim * 2, 'block2', reuse=reuse)\n",
        "            \"\"\" CONV BLOCK 3 \"\"\"\n",
        "            d_h2 = dis_block(d_h1, initial_dim * 2, initial_dim * 4, 'block3', reuse=reuse)\n",
        "            \"\"\" CONV BLOCK 4 \"\"\"\n",
        "            d_h3 = dis_block(d_h2, initial_dim * 4, initial_dim * 8, 'block4', reuse=reuse)\n",
        "            \"\"\" CONV BLOCK 5 \"\"\"\n",
        "            d_h4 = dis_block(d_h3, initial_dim * 8, 1, 'block5', reuse=reuse, normalize=False)\n",
        "            \"\"\" LINEAR BLOCK \"\"\"\n",
        "            d_h5 = linear(tf.reshape(d_h4, [self.batch_size, -1]), 1)\n",
        "        variables = tf.contrib.framework.get_variables(vs)\n",
        "        return d_h5, variables\n",
        "    \n",
        "    def get_feed_dict(self):\n",
        "        batch_z = np.random.normal(0, 1.0, size=[self.batch_size, self.z_dim]).astype(np.float32)\n",
        "        feed_dict = {self.z_vec: batch_z}\n",
        "        return feed_dict"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}